CODE OF MSD SYSTEM:

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from collections import deque
import random

# System Parameters
m, c, k = 1.0, 0.5, 2.0
dt = 0.1
actions = np.array([-1.0, 0.0, 1.0])  # Discrete action space

# DQN Parameters
gamma = 0.9
alpha = 0.001
epsilon = 1.0
epsilon_decay = 0.995
min_epsilon = 0.01
num_episodes = 1000
max_steps = 100
batch_size = 64

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Q-Network
class QNetwork(nn.Module):
    def __init__(self, input_dim=2, output_dim=3):
        super(QNetwork, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )

    def forward(self, x):
        return self.model(x)

# Initialize networks
policy_net = QNetwork().to(device)
target_net = QNetwork().to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=alpha)
criterion = nn.MSELoss()
replay_buffer = deque(maxlen=10000)

# Action selection
def select_action(state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.randint(len(actions))
    else:
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = policy_net(state_tensor)
        return torch.argmax(q_values).item()

# Store total error per episode
episode_errors = []

# Training loop
for episode in range(num_episodes):
    x, v = np.random.randn() * 0.5, np.random.randn() * 0.5
    total_error = 0

    for t in range(max_steps):
        state = np.array([x, v])
        action_idx = select_action(state, epsilon)
        u = actions[action_idx]

        # Corrected System dynamics
        a = (u - c * v - k * x) / m
        v_new = v + a * dt
        x_new = x + v * dt + 0.5 * a * dt ** 2
        next_state = np.array([x_new, v_new])

        reward = - (x**2 + v**2)
        total_error += x**2 + v**2
        done = abs(x) < 0.05 and abs(v) < 0.05

        # Store experience
        replay_buffer.append((state, action_idx, reward, next_state, done))

        # Learn from experience
        if len(replay_buffer) >= batch_size:
            batch = random.sample(replay_buffer, batch_size)
            states, actions_idx, rewards, next_states, dones = zip(*batch)

            states = torch.FloatTensor(np.array(states)).to(device)
            actions_idx = torch.LongTensor(actions_idx).unsqueeze(1).to(device)
            rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(device)
            next_states = torch.FloatTensor(np.array(next_states)).to(device)
            dones = torch.FloatTensor(np.array(dones).astype(np.float32)).unsqueeze(1).to(device)

            q_values = policy_net(states).gather(1, actions_idx)
            next_q_values = target_net(next_states).max(1, keepdim=True)[0]
            targets = rewards + gamma * next_q_values * (1 - dones)

            loss = criterion(q_values, targets.detach())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        x, v = x_new, v_new
        if done:
            break

    episode_errors.append(total_error)
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    if episode % 10 == 0:
        target_net.load_state_dict(policy_net.state_dict())

# Plot Error vs. Episode
plt.figure(figsize=(8, 5))
plt.plot(episode_errors)
plt.xlabel('Episode')
plt.ylabel('Total Error (Sum of x² + v²)')
plt.title('Error vs. Episode')
plt.grid(True)
plt.tight_layout()
plt.show()

# Test learned policy
x, v = 1.0, 0.0
x_hist, v_hist, u_hist = [], [], []
for _ in range(100):
    state = np.array([x, v])
    action_idx = select_action(state, epsilon=0.0)
    u = actions[action_idx]

    # Corrected dynamics
    a = (u - c * v - k * x) / m
    v = v + a * dt
    x = x + v * dt + 0.5 * a * dt ** 2

    x_hist.append(x)
    v_hist.append(v)
    u_hist.append(u)

# Plot results
plt.figure(figsize=(10, 8))

plt.subplot(3, 1, 1)
plt.plot(x_hist)
plt.grid()
plt.ylabel('Position (m)')
plt.title('Mass-Spring-Damper System Response (DQN)')

plt.subplot(3, 1, 2)
plt.plot(v_hist)
plt.grid()
plt.ylabel('Velocity (m/s)')

plt.subplot(3, 1, 3)
plt.step(range(len(u_hist)), u_hist)
plt.grid()
plt.ylabel('Control (N)')
plt.xlabel('Time Step')
plt.title('Control Force')

plt.tight_layout()
plt.show()
